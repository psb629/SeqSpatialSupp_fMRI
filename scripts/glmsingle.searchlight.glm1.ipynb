{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f1c2c03-097f-45a8-a1ef-8a52763e8daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import platform\n",
    "from os.path import join, exists, abspath, dirname\n",
    "from os import getcwd, makedirs\n",
    "from glob import glob\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colormaps, cm, colors\n",
    "import scipy\n",
    "from scipy.stats import ttest_1samp, pearsonr\n",
    "from scipy.io import loadmat\n",
    "from sklearn.metrics import r2_score\n",
    "import seaborn as sns\n",
    "import h5py\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae9f5f62-689a-4276-a0cd-0c4f05a8f6f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nibabel as nb\n",
    "from nilearn import plotting, image\n",
    "from nipype.interfaces import fsl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47742f37-bfce-44aa-b2ba-9a5c450f4fac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/sungbeenpark/github'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir_current = getcwd().replace('\\\\','/')\n",
    "\n",
    "tmp = dir_current.split('/')\n",
    "idx = [ii for ii, s in enumerate(tmp) if s=='github'][0]\n",
    "\n",
    "dir_git = '/'.join(tmp[:idx+1])\n",
    "dir_git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "495f3dda-caf5-4296-9182-1c9ea46cfefe",
   "metadata": {},
   "outputs": [],
   "source": [
    "dname = join(dir_git,'nitools')\n",
    "sys.path.append(dname)\n",
    "import nitools as nt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5385c6e4-1067-4715-954b-72c5b9252894",
   "metadata": {},
   "outputs": [],
   "source": [
    "dname = join(dir_git,'SUITPy')\n",
    "sys.path.append(dname)\n",
    "import SUITPy as suit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "91a5a34d-398c-4fae-8034-819db87def7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dname = join(dir_git)\n",
    "sys.path.append(dname)\n",
    "import surfAnalysisPy as surf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f4e8e268-fa6a-49ff-afc2-3528d00d210a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dname = join(dir_git,'Functional_Fusion')\n",
    "sys.path.append(dname)\n",
    "import Functional_Fusion.atlas_map as am\n",
    "import Functional_Fusion.reliability as rel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cc8058cb-d82d-49ff-9aca-f53cf353e9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "dname = join(dir_git,'AnatSearchlight')\n",
    "sys.path.append(dname)\n",
    "import AnatSearchlight.searchlight as sl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6f8001ea-2c76-4a7a-bc3f-aec68128aeb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dname = join(dir_git,'SeqSpatialSupp_fMRI')\n",
    "sys.path.append(dname)\n",
    "from SSS import deal_spm\n",
    "from SSS import util as su\n",
    "from SSS import plot as splt\n",
    "from SSS import image as simage\n",
    "from SSS import glmsingle as ssingle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da4f54ad-370e-4370-9fed-d88f76dc89c8",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "94e3dd40-880f-49d4-9879-bb84b2eee068",
   "metadata": {},
   "outputs": [],
   "source": [
    "border = simage.get_border(join(dir_git,'surfAnalysisPy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "523eab18-e303-496b-ab79-9e4ccbd7f5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_sn = su.get_list_sn()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3def2f-2952-4c34-adde-81469b73b63d",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "96900ab2-91a0-4c6e-ab47-add1338a5a3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/mnt/f/SeqSpatialSupp_fMRI/GLMsingle/glm_1'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glm = 1\n",
    "dir_glm = ssingle.get_dir_glmsingle(glm)\n",
    "dir_glm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e01b8f32-261e-496b-9839-e4f63e53dd62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir_work = join(dir_glm,'surfaceWB')\n",
    "exists(dir_work)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1061703-a8fe-4648-b2fb-912662e99dbe",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b10cdf92-517f-482c-ab13-2679ecbcd78a",
   "metadata": {},
   "source": [
    "To define a searchlight we need a:\n",
    "\n",
    "- Pial Surface (individual)\n",
    "- Gray Matter Mask (individual)\n",
    "- Functional mask image (mask.nii from GLM) telling us which voxels are availabe for searchlight analysis\n",
    "\n",
    "If we want to calculate the searchlight only for a part of a the surface, we can use a roi_specification (i.e. excluding the medial wall).\n",
    "\n",
    "The searchlight can be specified to have a specific radius in mm (on the surface) or a specific number of voxels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e95c0d-dcf5-40c4-9cc4-34d74f8ffccd",
   "metadata": {},
   "source": [
    "### Setup to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e380e09-4cee-4885-99bb-654410792492",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█████████████████████▎                                                                                               | 2/11 [16:28<1:15:04, 500.49s/it]"
     ]
    }
   ],
   "source": [
    "for ii, sn in enumerate(tqdm(list_sn)):\n",
    "    subj = 'S'+sn\n",
    "    \n",
    "    # These are the inndividual surfaces and the mask\n",
    "    white, pial, mask = simage.get_WPM(subj=subj, glm=glm)\n",
    "\n",
    "    # Optionally, you can define a roi-mask for the searchlight region on the surface. Centers outside of the mask will be ignored.\n",
    "    # Instead of a gifti-image you can also use a numpy array with the indices of the vertices that should be included in the searchlight.\n",
    "    roi_mask = join(dir_git,'AnatSearchlight/examples/tpl-fs32k_hemi-L_mask.label.gii')\n",
    "\n",
    "    # Give it an appropriate structure name: Used for generating cifti-files for output\n",
    "    mysearchlight = sl.SearchlightSurface('left_cortex')\n",
    "    \n",
    "    # Here we defining\n",
    "    if sn=='02':\n",
    "        maxradius = 60\n",
    "        maxvoxels = 70\n",
    "    else:\n",
    "        maxradius = 60\n",
    "        maxvoxels = 100\n",
    "    mysearchlight.define(\n",
    "        surfs=[pial,white],\n",
    "        mask_img=mask,\n",
    "        roi=roi_mask,\n",
    "        maxradius=maxradius,\n",
    "        maxvoxels=maxvoxels,\n",
    "        verbose=False\n",
    "    )\n",
    "\n",
    "    fname = join(dir_work,subj,'%s.h5'%subj)\n",
    "    mysearchlight.save(fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92894ff7-7ffc-4068-a844-83d4be015f9c",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f576f4d-e13f-40ea-8382-a8feb65ba53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mvpa_function(data, **kwargs):\n",
    "    Y = data\n",
    "    partition_vec = kwargs['partition_vec']\n",
    "    condition_vec = kwargs['condition_vec']\n",
    "\n",
    "    # within partition, average the data for same conditions:\n",
    "    unique_partitions = np.unique(partition_vec)\n",
    "    averaged_data = []\n",
    "    averaged_conditions = []\n",
    "    new_partition_vec = []\n",
    "    for part in unique_partitions:\n",
    "        part_indices = np.where(partition_vec == part)[0]\n",
    "        unique_conditions = np.unique(condition_vec[part_indices])\n",
    "        for cond in unique_conditions:\n",
    "            cond_indices = part_indices[condition_vec[part_indices] == cond]\n",
    "            averaged_data.append(np.mean(Y[cond_indices, :], axis=0))\n",
    "            averaged_conditions.append(cond)\n",
    "            new_partition_vec.append(part)\n",
    "    Y = np.array(averaged_data)\n",
    "    condition_vec = np.array(averaged_conditions)\n",
    "    partition_vec = np.array(new_partition_vec)\n",
    "\n",
    "    # ensure weird Y is not fed to pcm:\n",
    "    # Find columns that are all NaN\n",
    "    all_nan_cols = np.all(np.isnan(Y), axis=0)\n",
    "\n",
    "    # Find columns that are all zeros\n",
    "    all_zero_cols = np.all(Y == 0, axis=0)\n",
    "\n",
    "    # Combine: columns to remove (all NaN or all zeros)\n",
    "    cols_to_remove = all_nan_cols | all_zero_cols\n",
    "\n",
    "    # if cols_to_remove was not empty:\n",
    "    if np.any(cols_to_remove):\n",
    "        print('=================== found one! ===================')\n",
    "\n",
    "    Y = Y[:, ~cols_to_remove]\n",
    "\n",
    "    # if Y is empty:\n",
    "    if Y.size == 0:\n",
    "        return np.nan\n",
    "\n",
    "    \n",
    "    # estimate G matrix:\n",
    "    G_hat,_ = pcm.est_G_crossval(Y,\n",
    "                    condition_vec,\n",
    "                    partition_vec,\n",
    "                    X=pcm.matrix.indicator(partition_vec))\n",
    "    \n",
    "    D = pcm.G_to_dist(G_hat)\n",
    "    n = D.shape[0]\n",
    "\n",
    "    # heatmap of D:\n",
    "    fig,ax = plt.subplots(1,1,figsize=(3,2))\n",
    "    vmax_abs = np.max(np.abs(D))\n",
    "    sns.heatmap(D, ax=ax, cmap='RdBu_r', vmin=-vmax_abs, vmax=vmax_abs)\n",
    "    plt.show()\n",
    "    \n",
    "    out = np.sum(D) / (n**2 - n)\n",
    "    if np.isnan(out):\n",
    "        print('=================== found one! ===================')\n",
    "\n",
    "    # mean of distances:\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d32092-f98a-4ae0-95d7-e3f2b293ed7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the MVPA function that will be used in the searchlight\n",
    "# The functions take a n_images x n_voxels array as input.\n",
    "# and return either a scalar or vector as output.\n",
    "# Here we use simple example that calculate the mean activity in the searchlight region.\n",
    "def mvpa_function(Y, **kwargs):\n",
    "    partition_vec = kwargs['partition_vec']\n",
    "    condition_vec = kwargs['condition_vec']\n",
    "\n",
    "    # within partition, average the data for same conditions:\n",
    "    unique_partitions = np.unique(partition_vec)\n",
    "    averaged_data = []\n",
    "    averaged_conditions = []\n",
    "    new_partition_vec = []\n",
    "    for part in unique_partitions:\n",
    "        part_indices = np.where(partition_vec == part)[0]\n",
    "        unique_conditions = np.unique(condition_vec[part_indices])\n",
    "        for cond in unique_conditions:\n",
    "            cond_indices = part_indices[condition_vec[part_indices] == cond]\n",
    "            averaged_data.append(np.mean(Y[cond_indices, :], axis=0))\n",
    "            averaged_conditions.append(cond)\n",
    "            new_partition_vec.append(part)\n",
    "    Y = np.array(averaged_data)\n",
    "    condition_vec = np.array(averaged_conditions)\n",
    "    partition_vec = np.array(new_partition_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afac608f-3604-49c0-aeeb-4990e0e3f597",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a0ce54fc-9957-4448-8169-a82a324b9baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "subj = 'S01'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7e6d2733-8f41-4710-ae8f-3674fa9451df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average size of the searchlight: 9.7 mm\n",
      "Average number of voxels: 100.0\n"
     ]
    }
   ],
   "source": [
    "# You can now use the pre-computed searchlight.\n",
    "fname = join(dir_work,'%s.h5'%subj)\n",
    "mySearchlight = sl.load(fname)\n",
    "print(f'Average size of the searchlight: {mySearchlight.radius.mean():.1f} mm')\n",
    "print(f'Average number of voxels: {mySearchlight.nvoxels.mean():.1f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0665912-f380-4528-9f4c-730687f77ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8fd997-2b66-47f3-8541-6254247442a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run the searchlight with the defined scalar MVPA function\n",
    "results = mySearchlight.run(datafiles, mvpa_function, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d9194a-f61e-4cce-87b3-10ca02579b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "mySearchlight.data_to_cifti(results,outfilename='output.dscalar.nii')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e90f144e-7205-4134-bd6c-9fe39ce3f94d",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97750ed2-dcd5-44ea-a35c-3c95876b2bac",
   "metadata": {},
   "source": [
    "## Run mean beta searchlight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb208fd-c97b-4f31-b77b-3db704a13f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sn_list = [101, 102, 103, 104, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115]\n",
    "glm = 1\n",
    "hem = ['L', 'R']\n",
    "sl_name = ['left_cortex', 'right_cortex']\n",
    "conds = ['lhand', 'rhand', 'bi']\n",
    "\n",
    "def mean_function(data):\n",
    "    # mean_voxels = np.mean(data, axis=1)\n",
    "    # mean_beta = np.mean(mean_voxels, axis=0)\n",
    "    # std_beta = np.std(mean_voxels, axis=0, ddof=1)\n",
    "    # # t statistics:\n",
    "    # t_stats = (mean_beta) / (std_beta / np.sqrt(len(mean_voxels)))\n",
    "\n",
    "    mean_beta = np.nanmean(data[:-1, :])\n",
    "\n",
    "    return mean_beta\n",
    "\n",
    "for sn in sn_list:\n",
    "    try:\n",
    "        # run mean:\n",
    "        for cond in conds:\n",
    "                # regressor info:\n",
    "                reginfo = pd.read_table(os.path.join(baseDir, f'glm{glm}', f's{sn}', 'reginfo.tsv'))\n",
    "                # get indices of the regressors of interest:\n",
    "                regressors_of_interest = reginfo[reginfo.name.str.contains(cond)].index.tolist()\n",
    "\n",
    "                datafiles = [os.path.join(baseDir, f'glm{glm}', f's{sn}', f'beta_{i+1:04d}.nii') for i in regressors_of_interest]\n",
    "                datafiles.append(os.path.join(baseDir, f'glm{glm}', f's{sn}', 'ResMS.nii'))\n",
    "                \n",
    "                for i, h in enumerate(hem):\n",
    "                        S = sl.load(os.path.join(analysisDir,'searchlight',f'sl_{h}_s{sn}.h5'))\n",
    "                        results = S.run(datafiles, mean_function, verbose=False)\n",
    "                        S.data_to_cifti(results, outfilename=os.path.join(baseDir, 'searchlight', f'glm{glm}_{cond}_s{sn}_{h}_cortex.dscalar.nii'))\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing subject {sn}, {e}\")\n",
    "        continue\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce13a174-fa0d-4a03-976b-38dc4f4bb8a4",
   "metadata": {},
   "source": [
    "## Run MVPA searchlight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d82e71f-476a-4509-a311-b8dde25938a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/Users/aghavamp/Desktop/Projects/bimanual_wrist/data/fMRI/glmsingle/s101'\n",
    "\n",
    "# find all nifti files in the directory\n",
    "nifti_files = glob.glob(os.path.join(path, 'beta_*.nii'))\n",
    "\n",
    "# load the nifti files\n",
    "nifti_data = [nb.load(nifti_file) for nifti_file in nifti_files]\n",
    "\n",
    "# extract the data from the nifti files and make a (x,y,z,t) array\n",
    "data_arrays = [nifti.get_fdata() for nifti in nifti_data]\n",
    "# stack the data arrays along a new dimension\n",
    "data = np.stack(data_arrays, axis=-1)\n",
    "\n",
    "data.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc457ea-d2ab-4774-b0a8-527fe3102e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten the data to (x*y*z, t) shape\n",
    "data_flat = data.reshape(-1, data.shape[-1])\n",
    "data_flat.shape\n",
    "\n",
    "# how many rows do we have with full non nan values?\n",
    "non_nan_rows = ~np.isnan(data_flat).any(axis=1)\n",
    "num_non_nan_rows = np.sum(non_nan_rows)\n",
    "print(f\"Number of rows with all non NaN values: {num_non_nan_rows}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ab0242-42fd-4812-bb11-9b9e8f2be707",
   "metadata": {},
   "outputs": [],
   "source": [
    "sn_list = [101, 102, 103, 104, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115]\n",
    "glm = 'single'\n",
    "hem = ['L', 'R']\n",
    "sl_name = ['left_cortex', 'right_cortex']\n",
    "conds = ['lhand', 'rhand', 'bi']\n",
    "sn_list = [101]\n",
    "def mvpa_function(data, **kwargs):\n",
    "    Y = data\n",
    "    partition_vec = kwargs['partition_vec']\n",
    "    condition_vec = kwargs['condition_vec']\n",
    "\n",
    "    # within partition, average the data for same conditions:\n",
    "    unique_partitions = np.unique(partition_vec)\n",
    "    averaged_data = []\n",
    "    averaged_conditions = []\n",
    "    new_partition_vec = []\n",
    "    for part in unique_partitions:\n",
    "        part_indices = np.where(partition_vec == part)[0]\n",
    "        unique_conditions = np.unique(condition_vec[part_indices])\n",
    "        for cond in unique_conditions:\n",
    "            cond_indices = part_indices[condition_vec[part_indices] == cond]\n",
    "            averaged_data.append(np.mean(Y[cond_indices, :], axis=0))\n",
    "            averaged_conditions.append(cond)\n",
    "            new_partition_vec.append(part)\n",
    "    Y = np.array(averaged_data)\n",
    "    condition_vec = np.array(averaged_conditions)\n",
    "    partition_vec = np.array(new_partition_vec)\n",
    "\n",
    "    # ensure weird Y is not fed to pcm:\n",
    "    # Find columns that are all NaN\n",
    "    all_nan_cols = np.all(np.isnan(Y), axis=0)\n",
    "\n",
    "    # Find columns that are all zeros\n",
    "    all_zero_cols = np.all(Y == 0, axis=0)\n",
    "\n",
    "    # Combine: columns to remove (all NaN or all zeros)\n",
    "    cols_to_remove = all_nan_cols | all_zero_cols\n",
    "\n",
    "    # if cols_to_remove was not empty:\n",
    "    if np.any(cols_to_remove):\n",
    "        print('=================== found one! ===================')\n",
    "\n",
    "    Y = Y[:, ~cols_to_remove]\n",
    "\n",
    "    # if Y is empty:\n",
    "    if Y.size == 0:\n",
    "        return np.nan\n",
    "\n",
    "    \n",
    "    # estimate G matrix:\n",
    "    G_hat,_ = pcm.est_G_crossval(Y,\n",
    "                    condition_vec,\n",
    "                    partition_vec,\n",
    "                    X=pcm.matrix.indicator(partition_vec))\n",
    "    \n",
    "    D = pcm.G_to_dist(G_hat)\n",
    "    n = D.shape[0]\n",
    "\n",
    "    # heatmap of D:\n",
    "    fig,ax = plt.subplots(1,1,figsize=(3,2))\n",
    "    vmax_abs = np.max(np.abs(D))\n",
    "    sns.heatmap(D, ax=ax, cmap='RdBu_r', vmin=-vmax_abs, vmax=vmax_abs)\n",
    "    plt.show()\n",
    "    \n",
    "    out = np.sum(D) / (n**2 - n)\n",
    "    if np.isnan(out):\n",
    "        print('=================== found one! ===================')\n",
    "\n",
    "    # mean of distances:\n",
    "    return out\n",
    "\n",
    "for sn in sn_list:\n",
    "    # run MVPA:\n",
    "    for cond in conds:\n",
    "        print(f'Processing subject {sn}, condition {cond}...')\n",
    "        # regressor info:\n",
    "        reginfo = pd.read_table(os.path.join(baseDir, f'glm{glm}', f's{sn}', 'reginfo.tsv'))\n",
    "        # get indices of the regressors of interest:\n",
    "        regressors_of_interest = reginfo[(reginfo.name.str.contains(cond))].index.tolist()\n",
    "        \n",
    "        datafiles = [os.path.join(baseDir, f'glm{glm}', f's{sn}', f'beta_{i+1:04d}.nii') for i in regressors_of_interest]\n",
    "        # datafiles.append(os.path.join(baseDir, f'glm{glm}', f's{sn}', 'R2.nii'))\n",
    "        partition_vec = reginfo.loc[regressors_of_interest, 'run'].values\n",
    "        condition_vec = reginfo.loc[regressors_of_interest, 'name'].values\n",
    "\n",
    "        for i, h in enumerate(hem):\n",
    "            S = sl.load(os.path.join(analysisDir,'searchlight',f'sl_{h}_s{sn}.h5'))\n",
    "            results = S.run(datafiles, mvpa_function, {'partition_vec': partition_vec, 'condition_vec': condition_vec}, verbose=False)\n",
    "            S.data_to_cifti(results, outfilename=os.path.join(baseDir, 'searchlight', f'glm{glm}_mvpa_{cond}_s{sn}_{h}_cortex.dscalar.nii'))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca0c93de-f04e-45e1-8295-cef714b205ca",
   "metadata": {},
   "source": [
    "## Marginal left hand bimanual searchlight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb03f1b8-c18e-4c6e-8979-904d97b6b3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "sn_list = [101, 102, 103, 104, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115]\n",
    "glm = 1\n",
    "hem = ['L', 'R']\n",
    "sl_name = ['left_cortex', 'right_cortex']\n",
    "\n",
    "def mvpa_marginal_lhand_function(data, **kwargs):\n",
    "        Y_all = data[:-1, :] * np.sqrt(data[-1,:]/100)\n",
    "        Y_all = data[:-1,:] / np.sqrt(data[-1,:])\n",
    "        partitions_all = kwargs['partition_vec']\n",
    "        conditions_all = kwargs['condition_vec']\n",
    "        \n",
    "        # define the structure of the averaged data:\n",
    "        angles = [0, 60, 120, 180, 240, 300]\n",
    "        partitions = np.unique(partitions_all)\n",
    "        conditions = angles.copy()\n",
    "        Y = np.zeros((len(partitions)*len(conditions), Y_all.shape[1]))\n",
    "        partition_vec = np.repeat(partitions, len(conditions))\n",
    "        condition_vec = np.tile(conditions, len(partitions))\n",
    "        \n",
    "        # average the activity patterns and place into Y:\n",
    "        for i in range(Y.shape[0]):\n",
    "            what_partition = partition_vec[i]\n",
    "            what_condition = condition_vec[i]\n",
    "\n",
    "            name = f'bi:{what_condition}'\n",
    "            rows = [j for j, c in enumerate(conditions_all) if name in c and partitions_all[j] == what_partition]\n",
    "\n",
    "            # average the activity patterns:\n",
    "            Y[i, :] = np.mean(Y_all[rows, :], axis=0)\n",
    "            \n",
    "        # estimate the G matrix:\n",
    "        G_hat,_ = pcm.est_G_crossval(Y,\n",
    "                        condition_vec,\n",
    "                        partition_vec,\n",
    "                        X=pcm.matrix.indicator(partition_vec))\n",
    "        \n",
    "        D = pcm.G_to_dist(G_hat)\n",
    "        n = D.shape[0]\n",
    "        \n",
    "        # mean of distances:\n",
    "        return np.sum(D)/(n**2 - n)\n",
    "\n",
    "for sn in sn_list:\n",
    "    try:\n",
    "        # run marginal MVPA:\n",
    "        # regressor info:\n",
    "        reginfo = pd.read_table(os.path.join(baseDir, f'glm{glm}', f's{sn}', 'reginfo.tsv'))\n",
    "        # get indices of the regressors of interest:\n",
    "        regressors_of_interest = reginfo[reginfo.name.str.contains('bi')].index.tolist()\n",
    "\n",
    "        datafiles = [os.path.join(baseDir, f'glm{glm}', f's{sn}', f'beta_{i+1:04d}.nii') for i in regressors_of_interest]\n",
    "        datafiles.append(os.path.join(baseDir, f'glm{glm}', f's{sn}', 'ResMS.nii'))\n",
    "        partition_vec = reginfo.loc[regressors_of_interest, 'run'].values\n",
    "        condition_vec = reginfo.loc[regressors_of_interest, 'name'].values\n",
    "\n",
    "        for i, h in enumerate(hem):\n",
    "            S = sl.load(os.path.join(analysisDir,'searchlight',f'sl_{h}_s{sn}.h5'))\n",
    "            results = S.run(datafiles, mvpa_marginal_lhand_function, {'partition_vec': partition_vec, 'condition_vec': condition_vec})\n",
    "            S.data_to_cifti(results, outfilename=os.path.join(baseDir, 'searchlight', f'glm{glm}_marginal_lhand_s{sn}_{h}_cortex.dscalar.nii'))\n",
    "    \n",
    "    except:\n",
    "            print(f'Error processing subject {sn}. Skipping...')\n",
    "            continue\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bfa2d8f-0704-42ee-9492-d067cf451936",
   "metadata": {},
   "source": [
    "## test searchlight definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6a689c-d9a3-4b93-b808-4007cb76be7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "h = 'L'\n",
    "sn = 101\n",
    "\n",
    "S = sl.load(os.path.join(analysisDir,'searchlight',f'sl_{h}_s{sn}.h5'))\n",
    "\n",
    "S.center_indx\n",
    "S.voxel_indx\n",
    "voxlist = S.voxlist\n",
    "\n",
    "vertices = [5758, 5961]\n",
    "\n",
    "template_nii_path = os.path.join(baseDir, 'glmsingle', f's{sn}', 'mask.nii')\n",
    "mask = nb.load(template_nii_path)\n",
    "sz = mask.shape\n",
    "affine = mask.affine\n",
    "\n",
    "for vert in vertices:\n",
    "    center_mask = np.zeros(sz)\n",
    "    searchlight = np.transpose(S.voxel_indx[:, voxlist[vert]])\n",
    "    for i, j, k in searchlight:\n",
    "        center_mask[i, j, k] = 1\n",
    "\n",
    "    center_nii = nb.Nifti1Image(center_mask, affine=affine)\n",
    "    nb.save(center_nii, os.path.join(baseDir, 'searchlight', f'center_s{sn}_{h}_{vert}.nii'))\n",
    "\n",
    "# project search light to surf:\n",
    "surf_white = os.path.join(baseDir,surfacewbDir, f's{sn}', f's{sn}.{h}.white.32k.surf.gii')\n",
    "surf_pial = os.path.join(baseDir,surfacewbDir, f's{sn}', f's{sn}.{h}.pial.32k.surf.gii')\n",
    "surf_sulc = os.path.join(baseDir,surfacewbDir, f's{sn}', f's{sn}.{h}.sulc.32k.shape.gii')\n",
    "\n",
    "for vert in vertices:\n",
    "    # Map a Nifti to the surface of left hemisphere\n",
    "    DL = surf.map.vol_to_surf([os.path.join(baseDir, 'searchlight', f'center_s{sn}_{h}_{vert}.nii')], surf_pial, surf_white, stats='mode')\n",
    "\n",
    "    fig, axs = plt.subplots(figsize=(8, 8))\n",
    "    surf.plot.plotmap(DL, f'fs32k_L',\n",
    "                        underlay=None,\n",
    "                        underscale=[-1.5, 1],\n",
    "                        alpha=.7,\n",
    "                        cmap='coolwarm',\n",
    "                        overlay_type='label'\n",
    "                        )\n",
    "    plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb21dbe-c946-4e97-84e8-82690d9d7cca",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sss",
   "language": "python",
   "name": "sss"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
